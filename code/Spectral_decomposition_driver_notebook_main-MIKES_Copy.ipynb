{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Spectral_decomposition_driver_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The Isomantics algorithm consists of the following stages:\n",
    "### **(Stage 0 and 1)** Prepare vocabulary:\n",
    "* currently prepared languages:\n",
    "    1. English\n",
    "    2. Russian\n",
    "    3. German\n",
    "    4. French\n",
    "    5. Italian\n",
    "    6. Chinese \n",
    "    \n",
    "\n",
    "### **(Stage 2)** Train translation matrices:\n",
    "\n",
    "* Training set:\n",
    "    * For two given languages $Lg_1$ and $Lg_2$, we create a training set $\\Omega_{(Lg_1,Lg_2)}$ as follows:\n",
    "        1. For each $word_i$ in language 1, find the direct translation $\\widehat{word_i}$ in language 2.\n",
    "        2. Find vector embeddings $w_i\\in Lg_1$ and $\\widehat{w_i}\\in Lg_2$ of $word_i$ and $\\widehat{word_i}$ respectively.\n",
    "        3. Add the pair $<w_i,\\widehat{w_i}>$ to the training set $\\Omega_{(Lg_1,Lg_2)}$\n",
    "            * **Note** we found that training only for for only the the top 5-10k most popular terms in  $\\Omega_{(Lg_1,Lg_2)}$ generates the best word-to-word translation results on out of sample test sets.\n",
    "* Building the Cost function:\n",
    "    * Loss function for the learning process:\n",
    "        * $ Loss(T_{Lg_1,Lg_2})= ||Tw_i - \\widehat{w_i}||^2_2 $\n",
    "    * Regularization terms:\n",
    "        * Over fitting Regularizer:\n",
    "            * $Reg_{Frobenius}(T_{Lg_1,Lg_2}) = ||T_{Lg_1,Lg_2}||_2$\n",
    "        * Normality Regularizer:\n",
    "            * $Reg_{Normality}(T_{Lg_1,Lg_2}) = ||T_{Lg_1,Lg_2}^{T}T_{Lg_1,Lg_2} - T_{Lg_1,Lg_2}T_{Lg_1,Lg_2}^T||_2$\n",
    "                * **Note** The Normality Regularizer is used to ensure that the resulting matrix is diagonalizable.\n",
    "\n",
    "\n",
    "#### Full cost function:\n",
    "$$ J(T_{Lg_1,Lg_2})= Loss(T_{Lg_1,Lg_2}) + \\lambda_{1}Reg_{Frobenius}(T_{Lg_1,Lg_2}) + \\lambda_{2}Reg_{Normality}(T_{Lg_1,Lg_2}) $$  \n",
    "\n",
    "### **(Stage 3)** Translation Spectral Analysis:\n",
    "* Factor the matrix $T_{Lg_1,Lg_2} = U\\Sigma V^T$ where $U$ and $V$ are orthonormal (rotation) matrices and $\\Sigma$ gives the eigenvalues of $T_{Lg_1,Lg_2}$  or the \"*Translation spectrum*\"\n",
    "* Run a statistical analysis of the spectral values associated with each pair of languages.\n",
    "    1. mean\n",
    "    2. median\n",
    "    3. max value\n",
    "    4. min value\n",
    "    5. standard deviation\n",
    "\n",
    "* Compare the statistical spectral analysis across different language pairs.\n",
    ".***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loss Functions:\n",
    "##            $$ Loss(T_{Lg_1,Lg_2})= ||Tw_i - \\widehat{w_i}||^2_2 $$\n",
    "##            $$ Loss(T_{Lg_1,Lg_2})= cos(Tw_i,\\widehat{w_i}) $$\n",
    "\n",
    "\n",
    "# Regularization Terms\n",
    "## $$Reg_{Normality}(T_{Lg_1,Lg_2}) = ||T_{Lg_1,Lg_2}^{T}T_{Lg_1,Lg_2} - T_{Lg_1,Lg_2}T_{Lg_1,Lg_2}^T||_2$$\n",
    "* _Prevent singularity_\n",
    "## $$Reg_{Frobenius}(T_{Lg_1,Lg_2}) = ||T_{Lg_1,Lg_2}||_2$$\n",
    "* _Prevent overfitting_\n",
    "\n",
    "# Full cost function:\n",
    "## $$ J(T_{Lg_1,Lg_2})= Loss(T_{Lg_1,Lg_2}) + \\lambda_{1}Reg_{Frobenius}(T_{Lg_1,Lg_2}) + \\lambda_{2}Reg_{Normality}(T_{Lg_1,Lg_2}) $$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stages of Isomantics:\n",
    "\n",
    "***\n",
    "\n",
    "# Stage 0:\n",
    "\n",
    "### Download the files required for Isomantics.\n",
    "\n",
    "The Steps mentioned in **Stage 1** are used to:\n",
    "\n",
    "  1. Authenticating Google Drive API for translating words from one language to another. \n",
    "  2. Creating vocabulary and corresponding vectors pickle files for each language. \n",
    "  3. Translate one language vocab to another and create pickle files for lg1-> lg2 translations. \n",
    "  (e.g. en_en.pkl, en_ru.pkl....). \n",
    "  \n",
    "**Note: For the ease of running experiments and testing, pickle files for vocabs, vectors and translations have already been created.**\n",
    "\n",
    "* **Go to 1...** if you wish to create the pickles on your own from downloaded vectors. (takes a long time to create your own pickles)\n",
    "* **Go to 2...** if you wish to skip creating your own pickle translation files. ( Recommended)\n",
    "\n",
    "    1. To Download the Pre-Trained FastText Embeddings:\n",
    "          * Go to [facebookresearch/fasttext](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md)\n",
    "          * Click on the *[text]()* link corresponding to the language and save '.vec' file to '/code/fasttext' directory.\n",
    "          \n",
    "          </br>\n",
    "          **OR**\n",
    "          </br>\n",
    "          \n",
    "          * Vectors can be downloaded from the following links too.\n",
    "              * [English](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec)\n",
    "              * [Russian](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.ru.vec)\n",
    "              * [German](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.de.vec)\n",
    "              * [French](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec)\n",
    "              * [Italian](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.it.vec)\n",
    "              * [Chinese](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.zh.vec)\n",
    "          * Go to **Stage 0**.\n",
    "          \n",
    "       </br>   \n",
    "    2. Download the pickled translations from [pickled translations](https://mega.nz/#!wkRlQAAQ!bkqHMKfreAgo8jVJQywoAWOxjXHfM63WbfNx3nYHnQ4) link, save and unzip the folder in the root directory. (Don't change the name of the unzipped folder)  \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1:\n",
    "\n",
    "\n",
    "**1. If you've chosen to download the vectors, do the following steps to create your own .pkl translation files, if not go to 2.:**\n",
    "\n",
    "\n",
    "\n",
    "1. To authenticate the gdrive API:\n",
    "    * Go to [Google Drive API Guide](https://developers.google.com/drive/v3/web/quickstart/python) and follow the step 1 to turn on the gdrive API for your project.\n",
    "    * **Note**: Download the json file in step h) and name it as **client_secrets.json** in the root directory.\n",
    "2. Change directory to the **/code** and run the following command on the terminal to create a **gauth.yml** file:\n",
    "\n",
    "    ``` \n",
    "    $ python3 gauth.py \n",
    "    ```\n",
    "\n",
    "3. Run **vocab_vectors.py** to pickle the vocab and vector objects.\n",
    "\n",
    "    ```\n",
    "    $ python3 vocab_vectors.py\n",
    "    ```\n",
    "4. Run **build_translations.py** to create pickle translations for English to English.\n",
    "\n",
    "    ```\n",
    "    $ python3 build_translations.py en en\n",
    "\n",
    "    ```\n",
    "* open a new tab in the terminal and run the translations code for a new translation English to Spanish.\n",
    "\n",
    "    ```\n",
    "    $ python3 build_translations.py en es\n",
    "\n",
    "    ```\n",
    "        .........\n",
    "\n",
    "        .........\n",
    "\n",
    "    * and so on for all the possible combination of languages....\n",
    "\n",
    "    </br>\n",
    "        \n",
    "#### 2. If you've chosen to download the pickle files from the link mentioned, go directly to **Stage 2**        \n",
    "        \n",
    "        \n",
    "\n",
    "</br>\n",
    "***\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Stage 2) \n",
    "\n",
    "1. Train translation matrices:\n",
    "\n",
    " * the vocab and the vector embeddings are located in **'/pickle'** folder.\n",
    " * Change directory to **'/code'** to train the translation matrices.\n",
    " * The following code trains the transaltion matrices and exports the .csv files to the mentioned test folder in the **'/data'**\n",
    " * Run the following code with appropriate flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Creating Translation Matrices for....:\n",
      "\n",
      "en->en\n",
      "\n",
      "(5000, 300)\n",
      "en->ru\n",
      "\n",
      "(5000, 300)\n",
      "en->de\n",
      "\n",
      "(5000, 300)\n"
     ]
    }
   ],
   "source": [
    "# specify name of the experiment with '--e' flag: \n",
    "# specify the name of the model regularizer with '--m' flag\n",
    "\n",
    "#possible values for model regularizers: 'l2', 'l3_l2', 'l3'\n",
    "\n",
    "! python3 isomantics_train_translations.py --exp_name 100_cosine_proximity_l2_0_0001_normality_0_000001 --reg_name normality_l2 --loss_func cosine_proximity --dim 100 --l2_lambda 0.0001 --normality_lambda 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Stage 3) Translation Spectral Analysis\n",
    "\n",
    " * This code imports the translation matrices from the '/data/exp-name/T/' \n",
    " * Performs Spectral Analysis on the T matrices and exports the data for plotting heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Creating Translation Matrices for....:\n",
      "\n",
      "Exporting DataFrames for SVD Heatmaps as .CSV files\n"
     ]
    }
   ],
   "source": [
    "!python execute_spectral_analysis.py 100_cosine_proximity_l2_0_0001_normality_0_000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Stage 4) Create SVD Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ismtools import read_json, plot_heatmaps\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "JSON_PATH = \"../data/100_cosine_proximity_l2_0_0001_normality_0_000001/spec_analysis_stats.json\"\n",
    "MATRIX_TYPES = [\"T_matrix\", \"T_cov\"]\n",
    "STATS = ['min', \n",
    "         'max', \n",
    "         'mean', \n",
    "         'median', \n",
    "         'std', \n",
    "         'min_log', \n",
    "         'max_log', \n",
    "         'mean_log', \n",
    "         'median_log', \n",
    "#          'std_log', \n",
    "#          'ortho_norm',\n",
    "         'condition_num',\n",
    "         'log_condition_num',\n",
    "#          'determinant'\n",
    "#          'acc',\n",
    "         'fro',\n",
    "        ]\n",
    "\n",
    "\n",
    "heatmaps = plot_heatmaps(T_matrix_dict=read_json(JSON_PATH),\n",
    "                         plotted_stats=STATS,\n",
    "                         display_opt=\"cols\",\n",
    "                         matrix_types=MATRIX_TYPES,\n",
    "                         low_c=10,\n",
    "                         high_c=130,\n",
    "                         sep_num=80,\n",
    "                         figuresize=[12,9]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Translation Matrix Results  \n",
    "## En to Ru Fasttext_Random  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- Ru Vocabulary Size = 944,211  \n",
    "- Ru Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 3.9%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for Ru test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_ru_fasttext_random_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 32.3%  \n",
    "![](../images/en_ru_fasttext_random_T_isotropy.png)  \n",
    "\n",
    "## En to Ru Fasttext_Top  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- Ru Vocabulary Size = 944,211  \n",
    "- Ru Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 46.3%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for Ru test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_ru_fasttext_top_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 38.2%  \n",
    "![](../images/en_ru_fasttext_top_T_isotropy.png)  \n",
    "\n",
    "## En to De Fasttext_Random  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- De Vocabulary Size = 1,137,616  \n",
    "- De Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 21.9%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for De test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_de_fasttext_random_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 35.6%  \n",
    "![](../images/en_de_fasttext_random_T_isotropy.png)  \n",
    "\n",
    "## En to De Fasttext_Top  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- De Vocabulary Size = 1,137,616  \n",
    "- De Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 63.6%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for De test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_de_fasttext_top_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 43.4%  \n",
    "![](../images/en_de_fasttext_top_T_isotropy.png)  \n",
    "\n",
    "## En to It Zeroshot  \n",
    "- En Vocabulary Size = 200,000  \n",
    "- En Embedding Length = 300  \n",
    "- It Vocabulary Size = 200,000  \n",
    "- It Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,869  \n",
    "- <b>Test Accuracy = 27.9%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for It test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_it_zeroshot_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 46.6%  \n",
    "![](../images/en_it_zeroshot_T_isotropy.png)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
