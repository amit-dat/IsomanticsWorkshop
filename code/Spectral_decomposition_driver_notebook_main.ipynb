{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Spectral_decomposition_driver_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The Isomantics algorithm consists of the following stages:\n",
    "### **(Stage 0)** Prepare vocabulary\n",
    "* currently prepared languages:\n",
    "    1. English\n",
    "    2. Russian\n",
    "    3. German\n",
    "    4. French\n",
    "    5. Italian\n",
    "    6. Chinese \n",
    "    \n",
    "### **(Stage 1)** Word embeddings.  Using `fasttext` or `word2vec` we embed the vocab for each of the languages\n",
    "### **(Stage 2)** Train translation matrices:\n",
    "\n",
    "* Training set:\n",
    "    * For two given languages $Lg_1$ and $Lg_2$, we create a training set $\\Omega_{(Lg_1,Lg_2)}$ as follows:\n",
    "        1. For each $word_i$ in language 1, find the direct translation $\\widehat{word_i}$ in language 2.\n",
    "        2. Find vector embeddings $w_i\\in Lg_1$ and $\\widehat{w_i}\\in Lg_2$ of $word_i$ and $\\widehat{word_i}$ respectively.\n",
    "        3. Add the pair $<w_i,\\widehat{w_i}>$ to the training set $\\Omega_{(Lg_1,Lg_2)}$\n",
    "            * **Note** we found that training only for for only the the top 5-10k most popular terms in  $\\Omega_{(Lg_1,Lg_2)}$ generates the best word-to-word translation results on out of sample test sets.\n",
    "* Building the Cost function:\n",
    "    * Loss function for the learning process:\n",
    "        * $ Loss(T_{Lg_1,Lg_2})= ||Tw_i - \\widehat{w_i}||^2_2 $\n",
    "    * Regularization terms:\n",
    "        * Over fitting Regularizer:\n",
    "            * $Reg_{Frobenius}(T_{Lg_1,Lg_2}) = ||T_{Lg_1,Lg_2}||_2$\n",
    "        * Normality Regularizer:\n",
    "            * $Reg_{Normality}(T_{Lg_1,Lg_2}) = ||T_{Lg_1,Lg_2}^{T}T_{Lg_1,Lg_2} - T_{Lg_1,Lg_2}T_{Lg_1,Lg_2}^T||_2$\n",
    "                * **Note** The Normality Regularizer is used to ensure that the resulting matrix is diagonalizable.\n",
    "\n",
    "\n",
    "#### Full cost function:\n",
    "$$ J(T_{Lg_1,Lg_2})= Loss(T_{Lg_1,Lg_2}) + \\lambda_{1}Reg_{Frobenius}(T_{Lg_1,Lg_2}) + \\lambda_{2}Reg_{Normality}(T_{Lg_1,Lg_2}) $$  \n",
    "\n",
    "### **(Stage 3)** Translation Spectral Analysis:\n",
    "* Factor the matrix $T_{Lg_1,Lg_2} = U\\Sigma V^T$ where $U$ and $V$ are orthonormal (rotation) matrices and $\\Sigma$ gives the eigenvalues of $T_{Lg_1,Lg_2}$  or the \"*Translation spectrum*\"\n",
    "* Run a statistical analysis of the spectral values associated with each pair of languages.\n",
    "    1. mean\n",
    "    2. median\n",
    "    3. max value\n",
    "    4. min value\n",
    "    5. standard deviation\n",
    "\n",
    "* Compare the statistical spectral analysis across different language pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Stage 0) Prepare vocabulary:\n",
    "\n",
    "# TODO \n",
    " * add details on where the vocab was downloaded from\n",
    " * point to where the data is in the repo\n",
    " * add instructions on where to call it in the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Stages 1 and 2) \n",
    "1. Word embeddings. Using fasttext or word2vec we embed the vocab for each of the languages\n",
    "2. Train translation matrices:\n",
    "\n",
    "# TODO \n",
    " * add details on where the vocab is located\n",
    " * point to where the embeddings are located\n",
    " * Test following code on embedding process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import tools for running Spectral decomposition\n",
    "#import ismtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Creating Translation Matrices for....:\n",
      "\n",
      "en->en\n",
      "\n",
      "length of en-en vocab: 49999\n",
      "en->ru\n",
      "\n",
      "length of en-ru vocab: 15610\n",
      "en->de\n",
      "\n",
      "length of en-de vocab: 25405\n",
      "en->es\n",
      "\n",
      "length of en-es vocab: 20706\n",
      "en->fr\n",
      "\n",
      "length of en-fr vocab: 8658\n",
      "en->it\n",
      "\n",
      "length of en-it vocab: 4049\n",
      "en->zh-CN\n",
      "\n",
      "length of en-zh-CN vocab: 5611\n",
      "ru->en\n",
      "\n",
      "length of ru-en vocab: 37836\n",
      "ru->ru\n",
      "\n",
      "length of ru-ru vocab: 50000\n",
      "ru->de\n",
      "\n",
      "length of ru-de vocab: 39548\n",
      "ru->es\n",
      "\n",
      "length of ru-es vocab: 37071\n",
      "ru->fr\n",
      "\n",
      "length of ru-fr vocab: 3854\n",
      "ru->it\n",
      "\n",
      "length of ru-it vocab: 3905\n",
      "ru->zh-CN\n",
      "\n",
      "length of ru-zh-CN vocab: 16151\n",
      "de->en\n",
      "\n",
      "length of de-en vocab: 33478\n",
      "de->ru\n",
      "\n",
      "length of de-ru vocab: 21304\n",
      "de->de\n",
      "\n",
      "length of de-de vocab: 50000\n",
      "de->es\n",
      "\n",
      "length of de-es vocab: 28718\n",
      "de->fr\n",
      "\n",
      "length of de-fr vocab: 16381\n",
      "de->it\n",
      "\n",
      "length of de-it vocab: 7830\n",
      "de->zh-CN\n",
      "\n",
      "length of de-zh-CN vocab: 9575\n",
      "es->en\n",
      "\n",
      "length of es-en vocab: 20505\n",
      "es->ru\n",
      "\n",
      "length of es-ru vocab: 17038\n",
      "es->de\n",
      "\n",
      "length of es-de vocab: 24625\n",
      "es->es\n",
      "\n",
      "length of es-es vocab: 50000\n",
      "es->fr\n",
      "\n",
      "length of es-fr vocab: 8498\n",
      "es->it\n",
      "\n",
      "length of es-it vocab: 4178\n",
      "es->zh-CN\n",
      "\n",
      "length of es-zh-CN vocab: 7818\n",
      "fr->en\n",
      "\n",
      "length of fr-en vocab: 16551\n",
      "fr->ru\n",
      "\n",
      "length of fr-ru vocab: 4941\n",
      "fr->de\n",
      "\n",
      "length of fr-de vocab: 17494\n",
      "fr->es\n",
      "\n",
      "length of fr-es vocab: 12700\n",
      "fr->fr\n",
      "\n",
      "length of fr-fr vocab: 60000\n",
      "fr->it\n",
      "\n",
      "length of fr-it vocab: 9326\n",
      "fr->zh-CN\n",
      "\n",
      "length of fr-zh-CN vocab: 4331\n",
      "it->en\n",
      "\n",
      "length of it-en vocab: 6532\n",
      "it->ru\n",
      "\n",
      "length of it-ru vocab: 4455\n",
      "it->de\n",
      "\n",
      "length of it-de vocab: 10582\n",
      "it->es\n",
      "\n",
      "length of it-es vocab: 5891\n",
      "it->fr\n",
      "\n",
      "length of it-fr vocab: 8089\n",
      "it->it\n",
      "\n",
      "length of it-it vocab: 10000\n",
      "it->zh-CN\n",
      "\n",
      "length of it-zh-CN vocab: 2023\n",
      "zh-CN->en\n",
      "\n",
      "length of zh-CN-en vocab: 8773\n",
      "zh-CN->ru\n",
      "\n",
      "length of zh-CN-ru vocab: 7851\n",
      "zh-CN->de\n",
      "\n",
      "length of zh-CN-de vocab: 8889\n",
      "zh-CN->es\n",
      "\n",
      "length of zh-CN-es vocab: 8928\n",
      "zh-CN->fr\n",
      "\n",
      "length of zh-CN-fr vocab: 3554\n",
      "zh-CN->it\n",
      "\n",
      "length of zh-CN-it vocab: 4761\n",
      "zh-CN->zh-CN\n",
      "\n",
      "length of zh-CN-zh-CN vocab: 16380\n"
     ]
    }
   ],
   "source": [
    "# TODO?? put manual list of translations into ismtools or another imported .py\n",
    "\n",
    "# Default - Without Accuracy Statistic\n",
    "! python3 isomantics_train_translations.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Stage 3) Translation Spectral Analysis\n",
    "\n",
    "# TODO \n",
    " * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Creating Translation Matrices for....:\n",
      "\n",
      "Exporting DataFrames for SVD Heatmaps as .CSV files\n"
     ]
    }
   ],
   "source": [
    "! python3 isomantics_statistical_analysis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Stage 4) Create SVD Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! jupyter notebook SVD_heatmaps.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Translation Matrix Results  \n",
    "## En to Ru Fasttext_Random  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- Ru Vocabulary Size = 944,211  \n",
    "- Ru Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 3.9%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for Ru test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_ru_fasttext_random_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 32.3%  \n",
    "![](../images/en_ru_fasttext_random_T_isotropy.png)  \n",
    "\n",
    "## En to Ru Fasttext_Top  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- Ru Vocabulary Size = 944,211  \n",
    "- Ru Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 46.3%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for Ru test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_ru_fasttext_top_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 38.2%  \n",
    "![](../images/en_ru_fasttext_top_T_isotropy.png)  \n",
    "\n",
    "## En to De Fasttext_Random  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- De Vocabulary Size = 1,137,616  \n",
    "- De Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 21.9%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for De test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_de_fasttext_random_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 35.6%  \n",
    "![](../images/en_de_fasttext_random_T_isotropy.png)  \n",
    "\n",
    "## En to De Fasttext_Top  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- De Vocabulary Size = 1,137,616  \n",
    "- De Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 63.6%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for De test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_de_fasttext_top_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 43.4%  \n",
    "![](../images/en_de_fasttext_top_T_isotropy.png)  \n",
    "\n",
    "## En to It Zeroshot  \n",
    "- En Vocabulary Size = 200,000  \n",
    "- En Embedding Length = 300  \n",
    "- It Vocabulary Size = 200,000  \n",
    "- It Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,869  \n",
    "- <b>Test Accuracy = 27.9%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for It test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_it_zeroshot_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 46.6%  \n",
    "![](../images/en_it_zeroshot_T_isotropy.png)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
