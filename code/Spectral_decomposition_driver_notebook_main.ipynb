{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Spectral_decomposition_driver_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The Isomantics algorithm consists of the following stages:\n",
    "### **(Stage 0)** Prepare vocabulary\n",
    "* currently prepared languages:\n",
    "    1. English\n",
    "    2. Russian\n",
    "    3. German\n",
    "    4. French\n",
    "    5. Italian\n",
    "    6. Chinese \n",
    "    \n",
    "### **(Stage 1)** Word embeddings.  Using `fasttext` or `word2vec` we embed the vocab for each of the languages\n",
    "### **(Stage 2)** Train translation matrices:\n",
    "\n",
    "* Training set:\n",
    "    * For two given languages $Lg_1$ and $Lg_2$, we create a training set $\\Omega_{(Lg_1,Lg_2)}$ as follows:\n",
    "        1. For each $word_i$ in language 1, find the direct translation $\\widehat{word_i}$ in language 2.\n",
    "        2. Find vector embeddings $w_i\\in Lg_1$ and $\\widehat{w_i}\\in Lg_2$ of $word_i$ and $\\widehat{word_i}$ respectively.\n",
    "        3. Add the pair $<w_i,\\widehat{w_i}>$ to the training set $\\Omega_{(Lg_1,Lg_2)}$\n",
    "            * **Note** we found that training only for for only the the top 5-10k most popular terms in  $\\Omega_{(Lg_1,Lg_2)}$ generates the best word-to-word translation results on out of sample test sets.\n",
    "* Building the Cost function:\n",
    "    * Loss function for the learning process:\n",
    "        * $ Loss(T_{Lg_1,Lg_2})= ||Tw_i - \\widehat{w_i}||^2_2 $\n",
    "    * Regularization terms:\n",
    "        * Over fitting Regularizer:\n",
    "            * $Reg_{Frobenius}(T_{Lg_1,Lg_2}) = ||T_{Lg_1,Lg_2}||_2$\n",
    "        * Normality Regularizer:\n",
    "            * $Reg_{Normality}(T_{Lg_1,Lg_2}) = ||T_{Lg_1,Lg_2}^{T}T_{Lg_1,Lg_2} - T_{Lg_1,Lg_2}T_{Lg_1,Lg_2}^T||_2$\n",
    "                * **Note** The Normality Regularizer is used to ensure that the resulting matrix is diagonalizable.\n",
    "\n",
    "\n",
    "#### Full cost function:\n",
    "$$ J(T_{Lg_1,Lg_2})= Loss(T_{Lg_1,Lg_2}) + \\lambda_{1}Reg_{Frobenius}(T_{Lg_1,Lg_2}) + \\lambda_{2}Reg_{Normality}(T_{Lg_1,Lg_2}) $$  \n",
    "\n",
    "### **(Stage 3)** Translation Spectral Analysis:\n",
    "* Factor the matrix $T_{Lg_1,Lg_2} = U\\Sigma V^T$ where $U$ and $V$ are orthonormal (rotation) matrices and $\\Sigma$ gives the eigenvalues of $T_{Lg_1,Lg_2}$  or the \"*Translation spectrum*\"\n",
    "* Run a statistical analysis of the spectral values associated with each pair of languages.\n",
    "    1. mean\n",
    "    2. median\n",
    "    3. max value\n",
    "    4. min value\n",
    "    5. standard deviation\n",
    "\n",
    "* Compare the statistical spectral analysis across different language pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Stage 0) Prepare vocabulary:\n",
    "\n",
    "# TODO \n",
    " * add details on where the vocab was downloaded from\n",
    " * point to where the data is in the repo\n",
    " * add instructions on where to call it in the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Stages 1 and 2) \n",
    "1. Word embeddings. Using fasttext or word2vec we embed the vocab for each of the languages\n",
    "2. Train translation matrices:\n",
    "\n",
    "# TODO \n",
    " * add details on where the vocab is located\n",
    " * point to where the embeddings are located\n",
    " * Test following code on embedding process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import tools for running Spectral decomposition\n",
    "#import ismtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Creating Translation Matrices for....:\n",
      "\n",
      "en->en\n",
      "\n",
      "length of en-en vocab: 49999\n",
      "en->ru\n",
      "\n",
      "length of en-ru vocab: 15610\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# specify name of the experiment with '--e' flag: \n",
    "# specify the name of the model regularizer with '--m' flag\n",
    "\n",
    "#possible values for model regularizers: 'l2', 'l3_l2', 'l3'\n",
    "\n",
    "! python3 isomantics_train_translations.py --e exp_l2_T --m l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Stage 3) Translation Spectral Analysis\n",
    "\n",
    "# TODO \n",
    " * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Creating Translation Matrices for....:\n",
      "\n",
      "Exporting DataFrames for SVD Heatmaps as .CSV files\n"
     ]
    }
   ],
   "source": [
    "! python3 isomantics_statistical_analysis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Stage 4) Create SVD Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! jupyter notebook SVD_heatmaps.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Translation Matrix Results  \n",
    "## En to Ru Fasttext_Random  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- Ru Vocabulary Size = 944,211  \n",
    "- Ru Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 3.9%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for Ru test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_ru_fasttext_random_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 32.3%  \n",
    "![](../images/en_ru_fasttext_random_T_isotropy.png)  \n",
    "\n",
    "## En to Ru Fasttext_Top  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- Ru Vocabulary Size = 944,211  \n",
    "- Ru Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 46.3%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for Ru test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_ru_fasttext_top_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 38.2%  \n",
    "![](../images/en_ru_fasttext_top_T_isotropy.png)  \n",
    "\n",
    "## En to De Fasttext_Random  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- De Vocabulary Size = 1,137,616  \n",
    "- De Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 21.9%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for De test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_de_fasttext_random_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 35.6%  \n",
    "![](../images/en_de_fasttext_random_T_isotropy.png)  \n",
    "\n",
    "## En to De Fasttext_Top  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- De Vocabulary Size = 1,137,616  \n",
    "- De Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 63.6%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for De test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_de_fasttext_top_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 43.4%  \n",
    "![](../images/en_de_fasttext_top_T_isotropy.png)  \n",
    "\n",
    "## En to It Zeroshot  \n",
    "- En Vocabulary Size = 200,000  \n",
    "- En Embedding Length = 300  \n",
    "- It Vocabulary Size = 200,000  \n",
    "- It Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,869  \n",
    "- <b>Test Accuracy = 27.9%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for It test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_it_zeroshot_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 46.6%  \n",
    "![](../images/en_it_zeroshot_T_isotropy.png)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
