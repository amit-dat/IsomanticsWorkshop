{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Isomantics_workshop_driver_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline  Overview\n",
    "The Isomantics algorithm consists of the following stages:\n",
    "\n",
    "## **(Stage 0)** Gather Vocabularies and Training Labels:\n",
    "* This stage has been pre-completed for the workshop.    \n",
    "* More details on executing **Stage 0** on your own are located in the cells below. \n",
    "    * For full details on how data was prepared see [Michael Seeber's Capstone Report](https://github.com/mike-tamir/IsomanticsWorkshop/blob/master/reports/MichaelSeeber_Isomantics_report_capstone.pdf)\n",
    "\n",
    "\n",
    "## **(Stage 1)** Embed the Vocabularies:\n",
    "* This stage has been pre-completed for the workshop with FastText embeddings.  \n",
    "* More details on executing **Stage 1** on your own are located in the cells below. \n",
    "  \n",
    "## **(Stage 2)** Train translation matrices:\n",
    "* Training set:\n",
    "    * For two given languages $Lg_1$ and $Lg_2$, we create a training set $\\Omega_{(Lg_1,Lg_2)}$ as follows:\n",
    "        1. For each $word_i$ in language 1, find the direct translation $\\widehat{word_i}$ in language 2.\n",
    "        2. Find vector embeddings $w_i\\in Lg_1$ and $\\widehat{w_i}\\in Lg_2$ of $word_i$ and $\\widehat{word_i}$ respectively.\n",
    "        3. Add the pair $<w_i,\\widehat{w_i}>$ to the training set $\\Omega_{(Lg_1,Lg_2)}$\n",
    "            * **Note** we found that training only for for only the the top 5-10k most popular terms in  $\\Omega_{(Lg_1,Lg_2)}$ generates the best word-to-word translation results on out of sample test sets.\n",
    "\n",
    "### Loss Functions:\n",
    "###            $$ Loss(T_{Lg_1,Lg_2})= ||Tw_i - \\widehat{w_i}||^2_2 $$\n",
    "###            $$ Loss(T_{Lg_1,Lg_2})= cos(Tw_i,\\widehat{w_i}) $$\n",
    "\n",
    "\n",
    "### Regularization Terms\n",
    "### $$Reg_{Normality}(T_{Lg_1,Lg_2}) = ||T_{Lg_1,Lg_2}^{T}T_{Lg_1,Lg_2} - T_{Lg_1,Lg_2}T_{Lg_1,Lg_2}^T||_2$$\n",
    "* _To prevent training singular translation matrices_\n",
    "### $$Reg_{Frobenius}(T_{Lg_1,Lg_2}) = ||T_{Lg_1,Lg_2}||_2$$\n",
    "* _To prevent over-fitting_\n",
    "\n",
    "### Full cost function:\n",
    "### $$ J(T_{Lg_1,Lg_2})= Loss(T_{Lg_1,Lg_2}) + \\lambda_{1}Reg_{Frobenius}(T_{Lg_1,Lg_2}) + \\lambda_{2}Reg_{Normality}(T_{Lg_1,Lg_2}) $$  \n",
    "\n",
    "* Translation matrices have been pre-calculated and are stored in `IsomanticsWorkshop/data/[experiment_name]/T_matrices` as `csv` files.\n",
    "\n",
    "## **(Stage 3)** SVD and analysis of singular value stats:\n",
    "\n",
    "* Factor the matrix $T_{Lg_1,Lg_2} = U\\Sigma V^T$ where $U$ and $V$ are orthonormal (rotation) matrices and $\\Sigma$ gives the singular values (squared spectral values) of $T_{Lg_1,Lg_2}$.\n",
    "* Pull the statistics of the resulting singular values associated with each pair of languages.\n",
    "    1. `mean` and `mean_log`\n",
    "    2. `median` and `median_log`\n",
    "    3. `std` and `std_log`\n",
    "    4. `ortho_norm` \n",
    "        * Calculates $||T^{T}T-I||_{2}$ *\n",
    "            * A measure of closeness to the matrix orthogonality of $T_{Lg_1,Lg_2}$\n",
    "            * The $2$ subscript indicates the Frobenius norm.\n",
    "         \n",
    "    5. `condition_num` and `log_condition_num`\n",
    "        * $n_{c} := \\cfrac{\\lambda_{max}}{\\lambda_{min}}$\n",
    "            * $\\lambda_{max}$ and $\\lambda_{min}$ are the largest and smallest singular values respectively\n",
    "            * Acts as a useful measure of \"closeness to singularity\" of a matrix\n",
    "                * High $n_c$ means \"ill-conditioned\" or \"close to singularity\" (singular matrices have $n_c = \\infty$)\n",
    "                * Useful in the context of high dimensional matrices (as compared with direct determinants).\n",
    "\n",
    "## (Stage 4): Plot Heatmaps and Analyze Spectrum Statistics\n",
    "Plot as a heatmap grid the respective statistics associated with each translation.\n",
    "* The grid represents the translations from each language (row) to each other language (column).\n",
    "* Diagonals on the grid correspond to baselines for when a language is translated to itself.\n",
    "\n",
    "***\n",
    "***NOTE*** \n",
    "* *** Stages 0-2 take a significant amount of processing time.***\n",
    "* *** These stages can be executed on your own using the following steps.***\n",
    "* *** For now skip directly to stage 3 ***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Stage 3): Extract Translation Spectrum Statistics\n",
    "In Stage 3 we extract the spectra of the translation matrices and find statistical properties of the spectra.\n",
    "\n",
    "### Executed Stage 3 Steps:\n",
    "1. Read in all translation matrices generated and exported as csv files in Stage 2\n",
    "2. Calculate the spectral values and the log of the spectral values for:\n",
    "    1. the raw translation matrix and/or\n",
    "    2. the covariance matrix of the translation matrix (optional).\n",
    "3. Calculate statistical properties of the respective spectra.\n",
    "4. Export the resulting statistics as a json:\n",
    "    * Example `../data/dim_300_loss_mse_l2=0_01_normality=0_000001/spec_analysis_stats.json`\n",
    "\n",
    "Firs lets take a look at the available \"Experiments\" with translation matrices that have been pre-processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the available translation hyperparameter experiments\n",
    "experiments = !ls ../data\n",
    "print(\"These are the available already processed experiments:\")\n",
    "i=0\n",
    "for exp in experiments:\n",
    "    print(\"(Experiment %r): \\t%s\" % (i,exp))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage 3 is fully executed by running the following python script \n",
    "* `execute_spectral_analysis.py`\n",
    "* The `$experiment` argument specifies the experiment in `../data` with the `T_matrices` csv files exported in stage 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify which experiment in the list to run analysis on\n",
    "# you can change the index in the list experiments[1] to run analysis on a different experiment\n",
    "experiment = experiments[9]\n",
    "print(\"Calculating Spectrum statistics for the translation matrix from experiment:\\n\"+experiment+\"\\n\")\n",
    "\n",
    "# Run analysis with the execute_spectral_analysis.py script\n",
    "!python execute_spectral_analysis.py $experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Stage 4): Plot Heatmaps and Analyze Spectrum Statistics\n",
    "\n",
    "### Executed Stage 4 Steps:\n",
    "1. Read in the json containing the statistical properties of the spectra calculated in stage 3, \n",
    "2. Plot as a heatmap grid the respective statistics associated with each translation.\n",
    "    * The grid represents the translations from each language (row) to each other language (column).\n",
    "    * Diagonals on the grid correspond to baselines for when a language is translated to itself.\n",
    "    * Each heatmap is generated from \n",
    "        * a matrix type (translation matrix or its covariance matrix), and\n",
    "        * a different selected statistic.\n",
    "    \n",
    "Stage 4 can be executed in the below cell with the following two functions from `ismtools`:\n",
    "* `read_json` to read the json stats back in as the `T_matrix_dict`\n",
    "* `plot_heatmaps` select which heatmaps you want to generate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ismtools import read_json, plot_heatmaps\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Construct the JSON path based on the experiment chosen in stage 3\n",
    "experiment = experiments[9]\n",
    "PATH = \"../data/\"+experiment\n",
    "MATRIX_TYPES = [\n",
    "                \"T_matrix\",\n",
    "                \"T_cov\"\n",
    "               ]\n",
    "STATS = [\n",
    "         'mean', \n",
    "         'median', \n",
    "         'std', \n",
    "         'mean_log', \n",
    "         'median_log', \n",
    "         'std_log', \n",
    "         'ortho_norm',\n",
    "         'condition_num',\n",
    "         'log_condition_num'\n",
    "        ]\n",
    "\n",
    "\n",
    "heatmaps = plot_heatmaps(T_matrix_dict=read_json(PATH+\"/spec_analysis_stats.json\"),\n",
    "                         plotted_stats=STATS,\n",
    "                         display_opt=\"cols\",\n",
    "                         matrix_types=MATRIX_TYPES,\n",
    "                         low_c=10,\n",
    "                         high_c=140,\n",
    "                         sep_num=8,\n",
    "                         figuresize=[12,9],\n",
    "                         write_heatmaps=PATH\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing Stages (0-2) independently:\n",
    "\n",
    "## Executing Stage 0: \n",
    "### Download the files required for Isomantics.\n",
    "These steps are used to:\n",
    "\n",
    "  1. Authenticate the Google Drive API for translating words from one language to another. \n",
    "  2. Create vocabulary and corresponding vectors pickle files for each language. \n",
    "  3. Translate one language vocab to another and create pickle files for `lg1`-> `lg2` translations. \n",
    "  (e.g. `en_en.pkl`, `en_ru.pkl`...). \n",
    "  \n",
    "**Note: For the ease of running experiments and testing, pickle files for vocabs, vectors and translations have already been created.**\n",
    "To Download the Pre-Trained FastText Embeddings:\n",
    "* Go to [facebookresearch/fasttext](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md)\n",
    "* Click on the *[text]()* link corresponding to the language and save `.vec` file to `/code/fasttext` directory.\n",
    "\n",
    "</br>\n",
    "**OR**\n",
    "</br>\n",
    "\n",
    "* Vectors can be downloaded from the following links too.\n",
    "  * [English](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec)\n",
    "  * [Russian](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.ru.vec)\n",
    "  * [German](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.de.vec)\n",
    "  * [French](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec)\n",
    "  * [Italian](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.it.vec)\n",
    "  * [Chinese](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.zh.vec)\n",
    "          \n",
    "</br>   \n",
    "\n",
    "* Download the pickled translations from [pickled translations](https://mega.nz/#!wkRlQAAQ!bkqHMKfreAgo8jVJQywoAWOxjXHfM63WbfNx3nYHnQ4) link, save and unzip the folder in the root directory. (Don't change the name of the unzipped folder)  \n",
    "***\n",
    "\n",
    "## Stage 1:\n",
    "**1. If you've chosen to download the vectors, do the following steps to create your own .pkl translation files, if not go to 2.:**\n",
    "\n",
    "1. To authenticate the gdrive API:\n",
    "    * Go to [Google Drive API Guide](https://developers.google.com/drive/v3/web/quickstart/python) and follow the step 1 to turn on the gdrive API for your project.\n",
    "    * **Note**: Download the json file in step **h)** and name it as `client_secrets.json` in the root directory.\n",
    "2. Change directory to the `/code` and run the following command on the terminal to create a `gauth.yml` file:\n",
    "`$ python3 gauth.py`\n",
    "    \n",
    "3. Run **vocab_vectors.py** to pickle the vocab and vector objects.\n",
    "`$ python3 vocab_vectors.py`\n",
    "\n",
    "4. Run **build_translations.py** to create pickle translations for English to English.\n",
    "`$ python3 build_translations.py en en\n",
    "\n",
    "5. Open a new tab in the terminal and run the translations code for a new translation for (e.g.) English to Spanish.\n",
    "`$ python3 build_translations.py en es`\n",
    "    * Repeat for all other language combinations.\n",
    "\n",
    "</br>\n",
    "***\n",
    "\n",
    "## (Stage 2) \n",
    "\n",
    "### Train translation matrices:\n",
    "\n",
    " * The vocab and the vector embeddings are located in the created `IsomanticsWorkshop/pickle` dir.\n",
    " * Change directory to `IsomanticsWorkshop/code/` to train the translation matrices.\n",
    " * The following code trains the translation matrices and exports the `.csv` files to the specified experiment folder in `IsomanticsWorkshop/data/`\n",
    " * Run the following code with appropriate flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify name of the experiment with '--e' flag: \n",
    "# specify the name of the model regularizer with '--m' flag\n",
    "# possible values for model regularizers: 'l2', 'l3_l2', 'l3'\n",
    "! python3 isomantics_train_translations.py --exp_name 100_cosine_proximity_l2_0_0001_normality_0_000001 --reg_name normality_l2 --loss_func cosine_proximity --dim 100 --l2_lambda 0.0001 --normality_lambda 0.000001"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
