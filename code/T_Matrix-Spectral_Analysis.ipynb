{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# T_Matrix-Spectral_Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# %%writefile T_matrix_spectral_analysis_tools.py\n",
    "from __future__ import absolute_import\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ismtools import SVD, log, stat_calc, make_df\n",
    "\n",
    "def normality_val(matrix):\n",
    "    \"\"\"\n",
    "    matrix: numpy 2d array\n",
    "    RETURNS\n",
    "    A measure of the \"closeness to normality\" of matrix as follows: ||M*M - MM*||_2\n",
    "    where the norm is taken as the L2 norm for matrices and M is the matrix\n",
    "    Normal matrices should score 0.\n",
    "    \"\"\"\n",
    "    T = matrix\n",
    "    T_trans = matrix.transpose()\n",
    "    matrix_normality = np.linalg.norm(np.matrix(np.subtract(np.matmul(T,T_trans),np.matmul(T_trans,T))),'fro')\n",
    "    return matrix_normality\n",
    "\n",
    "\n",
    "def add_svd_stats(matrix, \n",
    "                  matrix_dict, \n",
    "                  stats, \n",
    "                  calc_SVD=False, \n",
    "                  log_spectrum=False,\n",
    "                  normality=False\n",
    "                 ):\n",
    "    \"\"\"\n",
    "    Adds selected stats to the T_matrix_dict and returns dict with added stats\n",
    "    matrix:        numpy array of the matrix to be processed to add to matrix_dict    \n",
    "    matrix_dict:    dictionary with matrix to be added to processing matrix\n",
    "    stats:          list statistics to store on the spectrum of the Tmatrix\n",
    "    log_spectrum:   Boolean if True (default) will run analysis on the log \n",
    "                    of the spectrum too.\n",
    "    \"\"\"\n",
    "    # Load the actual matrix\n",
    "    matrix_dict[\"matrix\"] = matrix\n",
    "    \n",
    "    # Load Normality of the matrix\n",
    "    if normality==True:\n",
    "        matrix_dict[\"normality\"] = normality_val(matrix)\n",
    "    \n",
    "    # Lod the SVD \n",
    "    if calc_SVD==True:\n",
    "        # Run SVD and add to sub keys\n",
    "        U,s,Vh = SVD(matrix)\n",
    "        matrix_dict[\"U_rotation\"] = U\n",
    "        matrix_dict[\"V_rotation_transpose\"] = Vh\n",
    "        matrix_dict[\"spectral_values\"] = s\n",
    "\n",
    "        # Conduct spectrum stats analysis\n",
    "        for stat in stats:\n",
    "            matrix_dict[stat] = stat_calc(stat, s, fro=None, acc=None)\n",
    "\n",
    "\n",
    "        # Add log spectrum\n",
    "        if log_spectrum==True:\n",
    "            sl = log(s)\n",
    "            matrix_dict[\"log_spectral_values\"]= sl\n",
    "\n",
    "            # Conduct log spectrum stats analysis\n",
    "            for stat in stats:\n",
    "                stat_of_log=stat+\"_log\"\n",
    "                matrix_dict[stat_of_log] = stat_calc(stat, sl, fro=None, acc=None)\n",
    "    \n",
    "    return matrix_dict\n",
    "\n",
    "\n",
    "def extract_T_matrix_dict(T_matrix_dir,\n",
    "                          stats=['min','max','mean','median','std'],\n",
    "                          calc_cov=True,\n",
    "                          calc_inv=False,\n",
    "                          calc_SVD=True,\n",
    "                          log_spectrum=True,\n",
    "                          normality=True,\n",
    "                          verbose=0\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    T_matrix_dir:  str location of the dir with the csv files for the translation matrices\n",
    "    \n",
    "    stats:         list of statistics to calculate on spectrum and log specturm if calc_SVD and \n",
    "                   log_spectrum are set to True respectively\n",
    "                   \n",
    "    calc_cov:      Boolean if default True, will process the covariance matrix and spectra in dict with key 'T_cov'\n",
    "    calc_inv:      Boolean if default True, will process the matrix inverse and spectra in dict with key 'T_inv'\n",
    "\n",
    "    calc_SVD:      Boolean if default True, will process the SVD of the T_matrix, and also calc_cov and calc_inv if \n",
    "                   they are set to true and add to the dict:\n",
    "                   \n",
    "                   U rotation numpy array 2d with key 'U_rotation'    \n",
    "                   V* rotation numpy array 2d with key 'V_rotation_trans' \n",
    "                   Spectral values list with key 'spectral_values'\n",
    "\n",
    "    \n",
    "    log_spectrum:  Boolean if default True, adds to dict log of the spectral values list with key 'log_spectral_values'\n",
    "    normality:     Boolean if default True, adds to dict the L2 norm of ||TT* - T*T|| with key 'normality'\n",
    "    \"\"\"\n",
    "    # create a list of items in T_matrix_dir\n",
    "    T_matrix_dir_items = os.listdir(T_matrix_dir)\n",
    "\n",
    "    # Loop through list of paths to process each matrix\n",
    "    T_matrix_dict = {}\n",
    "    for T_matrix_name in T_matrix_dir_items:\n",
    "\n",
    "        # Create path name\n",
    "        T_matrix_full_path = os.path.join(T_matrix_dir, T_matrix_name)\n",
    "\n",
    "        # Extract language strings and extensions\n",
    "        [T_matrix_lgs, T_matrix_ext] = T_matrix_name.split(\".\")\n",
    "        T_matrix_lgs = T_matrix_lgs.rstrip(\"_T\")\n",
    "\n",
    "        # Filter for non csv extensions\n",
    "        if T_matrix_ext != \"csv\":\n",
    "            if verbose >= 1:\n",
    "                print(\"Skipping %s item in %s: extension is not '.csv'\" % (T_matrix_name,T_matrix_dir))\n",
    "\n",
    "        else:\n",
    "            # Load in csv as numpy array\n",
    "            T_matrix = np.loadtxt(open(T_matrix_full_path), delimiter=\",\")\n",
    "\n",
    "            ### Add matrices to T_matrix_dict under Lg1-Lg-2 key\n",
    "            T_matrix_dict[T_matrix_lgs] = {}\n",
    "\n",
    "            # Initialize raw T_matrix key\n",
    "            T_matrix_dict[T_matrix_lgs][\"T_matrix\"] = {}\n",
    "            \n",
    "            # Load T_matrix and spectral analysis stats as configured into T_matrix sub dict\n",
    "            T_matrix_dict[T_matrix_lgs][\"T_matrix\"] = add_svd_stats(matrix=T_matrix, \n",
    "                                                                    matrix_dict=T_matrix_dict[T_matrix_lgs][\"T_matrix\"], \n",
    "                                                                    stats=stats, \n",
    "                                                                    calc_SVD=calc_SVD, \n",
    "                                                                    log_spectrum=log_spectrum,\n",
    "                                                                    normality=normality\n",
    "                                                                   )\n",
    "                                                                   \n",
    "            \n",
    "            # T_cov Covariance matrix TT^{T}\n",
    "            if calc_cov==True:\n",
    "                # calculate T_cov\n",
    "                T_cov = np.dot(T_matrix,T_matrix.T)\n",
    "                \n",
    "                # Initialize T_cov key\n",
    "                T_matrix_dict[T_matrix_lgs][\"T_cov\"] = {}\n",
    "                \n",
    "                # Load T_cov and spectral analysis stats as configured into T_cov sub dict\n",
    "                T_matrix_dict[T_matrix_lgs][\"T_cov\"] = add_svd_stats(matrix=T_cov, \n",
    "                                                                     matrix_dict=T_matrix_dict[T_matrix_lgs][\"T_cov\"], \n",
    "                                                                     stats=stats, \n",
    "                                                                     calc_SVD=calc_SVD, \n",
    "                                                                     log_spectrum=log_spectrum,\n",
    "                                                                     normality=normality\n",
    "                                                                    )\n",
    "                                                                    \n",
    "                \n",
    "                \n",
    "            # T_inv Matrix Inverse T^{-1}\n",
    "            if calc_inv==True:\n",
    "                # calculate T_inv\n",
    "                T_inv = np.linalg.inv(T_matrix)\n",
    "                \n",
    "                # Initialize T_inv key\n",
    "                T_matrix_dict[T_matrix_lgs][\"T_inv\"] = {}\n",
    "                \n",
    "                # Load T_inv and spectral analysis stats as configured into T_inv sub dict\n",
    "                T_matrix_dict[T_matrix_lgs][\"T_inv\"] = add_svd_stats(matrix=T_inv, \n",
    "                                                                     matrix_dict=T_matrix_dict[T_matrix_lgs][\"T_inv\"], \n",
    "                                                                     stats=stats, \n",
    "                                                                     calc_SVD=calc_SVD, \n",
    "                                                                     log_spectrum=log_spectrum,\n",
    "                                                                     normality=normality\n",
    "                                                                    )\n",
    "\n",
    "    return T_matrix_dict\n",
    "\n",
    "\n",
    "def make_heatmap(T_matrix_dict, \n",
    "                 matrix_type,\n",
    "                 stat, \n",
    "                 language_order= ['en','ru','de','es','fr','it', 'zh-CN'],\n",
    "                 upper_matrix_type= False\n",
    "                ):\n",
    "    \"\"\"\n",
    "    Makes a heatmap for the specifed stat and matrix_type from the T_matrix_dict\n",
    "    \n",
    "    matrix_dict:       dictionary with structure {'lg1_lg2':{\"matrix_type\":{stats:values}}}\n",
    "    matrix_type:       string in {'T_matrix', 'T_cov', 'T_inv'} to build the heatmap from\n",
    "    stat:              specific statistic to populate the heatmap\n",
    "    language_order:    order of the rows and columns of the heatmap\n",
    "    upper_matrix_type: Boolean or string, default False  \n",
    "                       If False, all values will be filled in with stats from matrix_type.\n",
    "                       If str in {'T_matrix', 'T_cov', 'T_inv'} upper matrix will be filled \n",
    "                       with stats from that matrix type\n",
    "    \"\"\"\n",
    "    # Initialize the heatmap df with 0s\n",
    "    df = make_df(language_order, language_order)\n",
    "    \n",
    "    # Loop through the language combos (or upper right combos)\n",
    "    for i in range(len(language_order)):\n",
    "        for j in range(len(language_order)):\n",
    "            lg1 = language_order[i]\n",
    "            lg2 = language_order[j]\n",
    "            \n",
    "            # Make language translation key from languages\n",
    "            lg_key = str(lg1)+\"_\"+str(lg2)\n",
    "            if lg_key not in T_matrix_dict.keys():\n",
    "                print(\"Skipping %s: translation stats not available\" % lg_key)\n",
    "            \n",
    "            # Pull value\n",
    "            value = T_matrix_dict[lg_key][matrix_type][stat]\n",
    "            \n",
    "            # Adjust upper vals if configured\n",
    "            if upper_matrix_type==False:\n",
    "                upper_value = value\n",
    "            else:\n",
    "                upper_value = T_matrix_dict[lg_key][upper_matrix_type][stat]\n",
    "            \n",
    "            # Update value based on upper tri config\n",
    "            if j<=i:                \n",
    "                df.set_value(lg1,lg2,value)\n",
    "            else:\n",
    "                df.set_value(lg1,lg2,upper_value)\n",
    "    return df\n",
    "\n",
    "def plot_heatmaps(T_matrix_dict,\n",
    "                  ploted_stats,\n",
    "                  display_opt=\"cols\",\n",
    "                  matrix_types=[\"T_matrix\"],\n",
    "                  low_c=10,\n",
    "                  high_c=130,\n",
    "                  sep_num=20,\n",
    "                 ):\n",
    "    \"\"\"\n",
    "    ###*** TODO: add documentation \n",
    "    \"\"\"\n",
    "\n",
    "    heatmaps={}\n",
    "    for matrix_type in matrix_types:\n",
    "        matrix_type_hmaps = {}\n",
    "        \n",
    "        # Initialize matrix_type plotting arguments\n",
    "        upper_matrix_type = False\n",
    "        matrix_type_used = matrix_type\n",
    "\n",
    "        # plot only upper triangle with T_inv\n",
    "        if matrix_type==\"T_inv\":\n",
    "            matrix_type_used = \"T_matrix\"\n",
    "            upper_matrix_type = \"T_inv\"\n",
    "            \n",
    "        for stat in ploted_stats:\n",
    "            heatmap = make_heatmap(T_matrix_dict=T_matrix_dict, \n",
    "                                   matrix_type=matrix_type_used,\n",
    "                                   stat=stat, \n",
    "                                   language_order= ['en','ru','de','es','fr','it', 'zh-CN'],\n",
    "                                   upper_matrix_type=upper_matrix_type\n",
    "                                  )\n",
    "\n",
    "            title = (matrix_type, stat)\n",
    "\n",
    "\n",
    "            if display_opt==\"cols\":\n",
    "\n",
    "                # Adjust colmap direction based on heatmap\n",
    "                mean_diag = np.diag(heatmap).mean()\n",
    "                hm_mean = np.array(heatmap).mean()\n",
    "\n",
    "                # TODO: Add more coloring options ###***###\n",
    "                low_c=low_c\n",
    "                high_c=high_c\n",
    "                sep_num=sep_num\n",
    "\n",
    "                if mean_diag>=hm_mean:\n",
    "                    colmap = sns.diverging_palette(low_c, high_c, sep=sep_num, as_cmap=True)\n",
    "                else:\n",
    "                    colmap = sns.diverging_palette(high_c, low_c, sep=sep_num, as_cmap=True)\n",
    "\n",
    "                hm = sns.heatmap(heatmap,cmap=colmap)\n",
    "                #title = hm.set_title(title)\n",
    "                print(title)\n",
    "\n",
    "                print(mean_diag, hm_mean, )\n",
    "                plt.show()\n",
    "                hm\n",
    "            else:\n",
    "                print(title)\n",
    "                display(heatmap)\n",
    "            matrix_type_hmaps[stat] = heatmap\n",
    "\n",
    "        # Add dict for that matrix type to full heatmap dict\n",
    "        heatmaps[matrix_type]=matrix_type_hmaps\n",
    "    \n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Make `extract_T_matrix_dict` do more than one matrix\n",
    "- test with writing to file\n",
    "\n",
    "- manage passing following lists\n",
    "    * `svds = ['s','s1']`\n",
    "    * `languages = ['en','ru','de','es','fr','it', 'zh-CN']`\n",
    "    * `stats = ['min','max','mean','median','std','fro','acc']`\n",
    "\n",
    "- EXPERIMENTS\n",
    "    * inverses in top right\n",
    "    * cov_matrix in the top right\n",
    "    * Normalizer of each matrix \n",
    "\n",
    "- add documentation to functions\n",
    "- streamline exporting heatmaps\n",
    "- change the seborn heatmap colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-054b89734d39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                              \u001b[0mlog_spectrum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                              \u001b[0mnormality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                              \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                             )\n\u001b[1;32m     23\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-519db2c5a3b9>\u001b[0m in \u001b[0;36mextract_T_matrix_dict\u001b[0;34m(T_matrix_dir, stats, calc_cov, calc_inv, calc_SVD, log_spectrum, normality, verbose)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# Load in csv as numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mT_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_matrix_full_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;31m### Add matrices to T_matrix_dict under Lg1-Lg-2 key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mtamir/anaconda/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0;31m# Convert each value according to its column and store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m             \u001b[0;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mtamir/anaconda/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0;31m# Convert each value according to its column and store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m             \u001b[0;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mtamir/anaconda/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mfloatconv\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34mb'0x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromhex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0mtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: "
     ]
    }
   ],
   "source": [
    "# from T_matrix_spectral_analysis_tools import *\n",
    "\n",
    "### Set args\n",
    "T_MATRIX_DIR = \"../data/HeatmapData/T/\"\n",
    "\n",
    "#\"../data/test_l3_l2_T/T/\"\n",
    "\n",
    "VERBOSE = 3\n",
    "#######\n",
    "\"\"\"\n",
    "td = extract_T_matrix_dict(T_matrix_dir=T_MATRIX_DIR, verbose=VERBOSE)\n",
    "print(td)\"\"\"\n",
    "\n",
    "full = extract_T_matrix_dict(T_matrix_dir=T_MATRIX_DIR,\n",
    "                             stats=['min','max','mean','median','std'],\n",
    "                             calc_cov=True,\n",
    "                             calc_inv=True,\n",
    "                             calc_SVD=True,\n",
    "                             log_spectrum=True,\n",
    "                             normality=True,\n",
    "                             verbose=0\n",
    "                            )\n",
    "full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Seaborn for plotting and styling\n",
    "import seaborn as sns\n",
    "# Matplotlib for additional customization\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#cmap = sns.diverging_palette(10, 130, sep=20, as_cmap=True)\n",
    "# cmap1 = create_palette(130, 10, 20, True)\n",
    "cmap = sns.diverging_palette(10, 130, sep=20, as_cmap=True)\n",
    "cmap1 = sns.diverging_palette(130, 10, sep=20, as_cmap=True)\n",
    "\n",
    "\n",
    "###^ Graphing stuff ^ ###\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "##############\n",
    "\n",
    "#Set variables\n",
    "T_MATRIX_DICT = full\n",
    "DISPLAY_OPT = \"cols\"\n",
    "STATS = ['normality', 'min', 'max', 'mean', 'median', 'std', 'min_log', 'max_log', 'mean_log', 'median_log', 'std_log']\n",
    "MATRIX_TYPES = [\"T_matrix\", \"T_cov\",\"T_inv\"]\n",
    "\n",
    "\n",
    "###\n",
    "heatmaps = plot_heatmaps(T_matrix_dict=T_MATRIX_DICT,\n",
    "                         ploted_stats=STATS,\n",
    "                         display_opt=\"cols\",\n",
    "                         matrix_types=MATRIX_TYPES,\n",
    "                         low_c=10,\n",
    "                         high_c=130,\n",
    "                         sep_num=20\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "***** NOTE THE BELOW HAS BEEN MOVED TO DRIVER NOTEBOOK CURRENTLY IN CODE *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The Isomantics algorithm consists of the following stages:\n",
    "### **(Stage 0)** Prepare vocabulary\n",
    "* currently prepared languages:\n",
    "    1. English\n",
    "    2. Russian\n",
    "    3. German\n",
    "    4. French\n",
    "    5. Italian\n",
    "    6. Chinese \n",
    "    \n",
    "### **(Stage 1)** Word embeddings.  Using `fasttext` or `word2vec` we embed the vocab for each of the languages\n",
    "### **(Stage 2)** Train translation matrices:\n",
    "\n",
    "* Training set:\n",
    "    * For two given languages $Lg_1$ and $Lg_2$, we create a training set $\\Omega_{(Lg_1,Lg_2)}$ as follows:\n",
    "        1. For each $word_i$ in language 1, find the direct translation $\\widehat{word_i}$ in language 2.\n",
    "        2. Find vector embeddings $w_i\\in Lg_1$ and $\\widehat{w_i}\\in Lg_2$ of $word_i$ and $\\widehat{word_i}$ respectively.\n",
    "        3. Add the pair $<w_i,\\widehat{w_i}>$ to the training set $\\Omega_{(Lg_1,Lg_2)}$\n",
    "            * **Note** we found that training only for for only the the top 5-10k most popular terms in  $\\Omega_{(Lg_1,Lg_2)}$ generates the best word-to-word translation results on out of sample test sets.\n",
    "* Building the Cost function:\n",
    "    * Loss function for the learning process:\n",
    "        * $ Loss(T_{Lg_1,Lg_2})= ||Tw_i - \\widehat{w_i}||^2_2 $\n",
    "    * Regularization terms:\n",
    "        * Over fitting Regularizer:\n",
    "            * $Reg_{Frobenius}(T_{Lg_1,Lg_2}) = ||T_{Lg_1,Lg_2}||_2$\n",
    "        * Normality Regularizer:\n",
    "            * $Reg_{Normality}(T_{Lg_1,Lg_2}) = ||T_{Lg_1,Lg_2}^{T}T_{Lg_1,Lg_2} - T_{Lg_1,Lg_2}T_{Lg_1,Lg_2}^T||_2$\n",
    "                * **Note** The Normality Regularizer is used to ensure that the resulting matrix is diagonalizable.\n",
    "\n",
    "\n",
    "#### Full cost function:\n",
    "$$ J(T_{Lg_1,Lg_2})= Loss(T_{Lg_1,Lg_2}) + \\lambda_{1}Reg_{Frobenius}(T_{Lg_1,Lg_2}) + \\lambda_{2}Reg_{Normality}(T_{Lg_1,Lg_2}) $$  \n",
    "\n",
    "### **(Stage 3)** Translation Spectral Analysis:\n",
    "* Factor the matrix $T_{Lg_1,Lg_2} = U\\Sigma V^T$ where $U$ and $V$ are orthonormal (rotation) matrices and $\\Sigma$ gives the eigenvalues of $T_{Lg_1,Lg_2}$  or the \"*Translation spectrum*\"\n",
    "* Run a statistical analysis of the spectral values associated with each pair of languages.\n",
    "    1. mean\n",
    "    2. median\n",
    "    3. max value\n",
    "    4. min value\n",
    "    5. standard deviation\n",
    "\n",
    "* Compare the statistical spectral analysis across different language pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Stage 0) Prepare vocabulary:\n",
    "\n",
    "# TODO \n",
    " * add details on where the vocab was downloaded from\n",
    " * point to where the data is in the repo\n",
    " * add instructions on where to call it in the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Stages 1 and 2) \n",
    "1. Word embeddings. Using fasttext or word2vec we embed the vocab for each of the languages\n",
    "2. Train translation matrices:\n",
    "\n",
    "# TODO \n",
    " * add details on where the vocab is located\n",
    " * point to where the embeddings are located\n",
    " * Test following code on embedding process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import tools for running Spectral decomposition\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import ismtools\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO script this section and call the script from bash in this cell\n",
    "    # add the -a for setting calculate_KNN = True\n",
    "# TODO?? put manual list of translations into ismtools or another imported .py\n",
    "\n",
    "#Set parameter on calculate KNN (need to change for -a sysarg)\n",
    "calculate_KNN = False\n",
    "\n",
    "languages = ['en','ru','de','es','fr','it', 'zh-CN']\n",
    "for lang1 in languages:\n",
    "    for lang2 in languages:\n",
    "        translations.append(('fasttext_top',lang1, lang2))\n",
    "\n",
    "for translation in translations:\n",
    "    embedding, lg1, lg2 = translation\n",
    "    # Vocab/Vectors/Dicts\n",
    "    lg1_vocab, lg1_vectors, lg2_vocab, lg2_vectors = \\\n",
    "        pickle_rw((lg1 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                  (lg1 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                  (lg2 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                  (lg2 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                  write=False)\n",
    "    lg1_dict = make_dict(lg1_vocab, lg1_vectors)\n",
    "    lg2_dict = make_dict(lg2_vocab, lg2_vectors)\n",
    "\n",
    "    print('Translation: '+lg1+'->'+lg2+'\\n')\n",
    "\n",
    "    # Train/Test Vocab/Vectors\n",
    "    vocab_train, vocab_test = vocab_train_test(embedding, lg1, lg2, lg1_vocab)\n",
    "    X_train, X_test, y_train, y_test = vectors_train_test(vocab_train,\n",
    "                                                          vocab_test,lg1_dict,lg2_dict)\n",
    "    \n",
    "    # Fit tranlation matrix to training data\n",
    "    model, history, T, tf, I, M, fro = translation_matrix(X_train, y_train)\n",
    "    \n",
    "    if calculate_KNN:\n",
    "        results_df = translation_results(X_test, y_test, vocab_test, T,\n",
    "                                     lg2_vectors, lg2_vocab)\n",
    "        acc = T_norm_EDA(results_df)\n",
    "\n",
    "\"\"\"\n",
    "TODO create standardized dumping location for the translation matrix and its meta data.\n",
    "- pickel dump based on the dir structure in the folders\n",
    "\"\"\"        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Stage 3) Translation Spectral Analysis\n",
    "\n",
    "# TODO \n",
    " * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./T_Matrices_examples/T_matrix_de_es.pkl\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-85616075e742>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"./T_Matrices_examples/T_matrix_de_es.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mobject_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \"\"\"\n",
      "\u001b[0;32m/Users/mtamir/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mtamir/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mtamir/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload_eof\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_eof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_eof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set list of T_matrix full paths\n",
    "T_matrix_dir = \"./T_Matrices_examples/\"\n",
    "T_matrix_names = !ls $T_matrix_dir\n",
    "\n",
    "T_matrix_full_paths = []\n",
    "for T_matrix_name in T_matrix_names:\n",
    "    T_matrix_loc = T_matrix_dir+T_matrix_name\n",
    "    T_matrix_full_paths += [T_matrix_loc]\n",
    "\n",
    "# Load T_matrix into memory\n",
    "load_path = T_matrix_full_paths[2]\n",
    "print(load_path)\n",
    "file = open (\"./T_Matrices_examples/T_matrix_de_es.pkl\",'rb')\n",
    "file.seek(0)\n",
    "object_file = pickle.load(file)\n",
    "\n",
    "\"\"\"\n",
    "with open(r\"./T_Matrices_examples/T_matrix_de_es.pkl\", 'rb') as f:\n",
    "    T_matrix = pickle.load(f)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U,s,Vh = SVD(T)\n",
    "\n",
    "s1 = log(s)\n",
    "\n",
    "\n",
    "for stat in stats:\n",
    "    svd[stat,translation[1],translation[2]] = stat_calc(stat, s, fro, acc)\n",
    "    svd1[stat,translation[1],translation[2]] = stat_calc(stat, s1, fro, acc)\n",
    "\n",
    "\n",
    "#Exporting DataFrames for SVD Heatmaps\n",
    "\n",
    "s_df = make_df(languages,languages)\n",
    "s1_df = make_df(languages,languages)\n",
    "\n",
    "\n",
    "for stat in stats:\n",
    "    for lang1 in languages:\n",
    "        for lang2 in languages:\n",
    "            s_df.set_value(lang1,lang2,svd[stat,lang1,lang2])\n",
    "            s1_df.set_value(lang1,lang2,svd1[stat,lang1,lang2])\n",
    "\n",
    "    s_df.to_csv('../HeatmapData/T/s_{}.csv'.format(stat),columns = languages)\n",
    "    s1_df.to_csv('../HeatmapData/T/s1_{}.csv'.format(stat),columns = languages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Manually set list of translations (embedding, lg1, lg2)\n",
    "    \n",
    "    svds = ['s','s1']\n",
    "    languages = ['en','ru','de','es','fr','it', 'zh-CN']\n",
    "    stats = ['min','max','mean','median','std','fro','acc']\n",
    "    \n",
    "    translations=[]\n",
    "    \n",
    "    for lang1 in languages:\n",
    "        for lang2 in languages:\n",
    "            translations.append(('fasttext_top',lang1, lang2))\n",
    "    \n",
    "    svd = {}\n",
    "    svd1 = {}\n",
    "    \n",
    "    for translation in translations:\n",
    "        embedding, lg1, lg2 = translation\n",
    "        # Vocab/Vectors/Dicts\n",
    "        lg1_vocab, lg1_vectors, lg2_vocab, lg2_vectors = \\\n",
    "            pickle_rw((lg1 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                      (lg1 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                      (lg2 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                      (lg2 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                      write=False)\n",
    "        lg1_dict = make_dict(lg1_vocab, lg1_vectors)\n",
    "        lg2_dict = make_dict(lg2_vocab, lg2_vectors)\n",
    "\n",
    "        print('Translation: '+lg1+'->'+lg2+'\\n')\n",
    "\n",
    "        # Train/Test Vocab/Vectors\n",
    "        vocab_train, vocab_test = vocab_train_test(embedding, lg1, lg2, lg1_vocab)\n",
    "        X_train, X_test, y_train, y_test = vectors_train_test(vocab_train,\n",
    "                                                              vocab_test,lg1_dict,lg2_dict)\n",
    " \n",
    "        \n",
    "        # Fit tranlation matrix to training data\n",
    "        model, history, T, tf,I, M, fro = translation_matrix(X_train, y_train)\n",
    "        \n",
    "        results_df = translation_results(X_test, y_test, vocab_test, T,\n",
    "                                         lg2_vectors, lg2_vocab)\n",
    "        acc = T_norm_EDA(results_df)\n",
    "        \n",
    "        U,s,Vh = SVD(T)\n",
    "        \n",
    "        s1 = log(s)\n",
    "    \n",
    "        \n",
    "        for stat in stats:\n",
    "            svd[stat,translation[1],translation[2]] = stat_calc(stat, s, fro, acc)\n",
    "            svd1[stat,translation[1],translation[2]] = stat_calc(stat, s1, fro, acc)\n",
    "            \n",
    "        \n",
    "    #Exporting DataFrames for SVD Heatmaps\n",
    "    \n",
    "    s_df = make_df(languages,languages)\n",
    "    s1_df = make_df(languages,languages)\n",
    "    \n",
    "\n",
    "    for stat in stats:\n",
    "        for lang1 in languages:\n",
    "            for lang2 in languages:\n",
    "                s_df.set_value(lang1,lang2,svd[stat,lang1,lang2])\n",
    "                s1_df.set_value(lang1,lang2,svd1[stat,lang1,lang2])\n",
    "\n",
    "        s_df.to_csv('../HeatmapData/T/s_{}.csv'.format(stat),columns = languages)\n",
    "        s1_df.to_csv('../HeatmapData/T/s1_{}.csv'.format(stat),columns = languages)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Translation Matrix Results  \n",
    "## En to Ru Fasttext_Random  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- Ru Vocabulary Size = 944,211  \n",
    "- Ru Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 3.9%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for Ru test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_ru_fasttext_random_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 32.3%  \n",
    "![](../images/en_ru_fasttext_random_T_isotropy.png)  \n",
    "\n",
    "## En to Ru Fasttext_Top  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- Ru Vocabulary Size = 944,211  \n",
    "- Ru Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 46.3%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for Ru test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_ru_fasttext_top_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 38.2%  \n",
    "![](../images/en_ru_fasttext_top_T_isotropy.png)  \n",
    "\n",
    "## En to De Fasttext_Random  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- De Vocabulary Size = 1,137,616  \n",
    "- De Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 21.9%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for De test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_de_fasttext_random_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 35.6%  \n",
    "![](../images/en_de_fasttext_random_T_isotropy.png)  \n",
    "\n",
    "## En to De Fasttext_Top  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- De Vocabulary Size = 1,137,616  \n",
    "- De Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 63.6%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for De test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_de_fasttext_top_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 43.4%  \n",
    "![](../images/en_de_fasttext_top_T_isotropy.png)  \n",
    "\n",
    "## En to It Zeroshot  \n",
    "- En Vocabulary Size = 200,000  \n",
    "- En Embedding Length = 300  \n",
    "- It Vocabulary Size = 200,000  \n",
    "- It Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,869  \n",
    "- <b>Test Accuracy = 27.9%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for It test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_it_zeroshot_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 46.6%  \n",
    "![](../images/en_it_zeroshot_T_isotropy.png)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
