{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Spectral_decomposition_driver_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The Isomantics algorithm consists of the following stages:\n",
    "### **(Stage 0)** Prepare vocabulary\n",
    "* currently prepared languages:\n",
    "    1. English\n",
    "    2. Russian\n",
    "    3. German\n",
    "    4. French\n",
    "    5. Italian\n",
    "    6. Chinese \n",
    "    \n",
    "### **(Stage 1)** Word embeddings.  Using `fasttext` or `word2vec` we embed the vocab for each of the languages\n",
    "### **(Stage 2)** Train translation matrices:\n",
    "\n",
    "* Training set:\n",
    "    * For two given languages $Lg_1$ and $Lg_2$, we create a training set $\\Omega_{(Lg_1,Lg_2)}$ as follows:\n",
    "        1. For each $word_i$ in language 1, find the direct translation $\\widehat{word_i}$ in language 2.\n",
    "        2. Find vector embeddings $w_i\\in Lg_1$ and $\\widehat{w_i}\\in Lg_2$ of $word_i$ and $\\widehat{word_i}$ respectively.\n",
    "        3. Add the pair $<w_i,\\widehat{w_i}>$ to the training set $\\Omega_{(Lg_1,Lg_2)}$\n",
    "            * **Note** we found that training only for for only the the top 5-10k most popular terms in  $\\Omega_{(Lg_1,Lg_2)}$ generates the best word-to-word translation results on out of sample test sets.\n",
    "* Building the Cost function:\n",
    "    * Loss function for the learning process:\n",
    "        * $ Loss(T_{Lg_1,Lg_2})= ||Tw_i - \\widehat{w_i}||^2_2 $\n",
    "    * Regularization terms:\n",
    "        * Over fitting Regularizer:\n",
    "            * $Reg_{Frobenius}(T_{Lg_1,Lg_2}) = ||T_{Lg_1,Lg_2}||_2$\n",
    "        * Normality Regularizer:\n",
    "            * $Reg_{Normality}(T_{Lg_1,Lg_2}) = ||T_{Lg_1,Lg_2}^{T}T_{Lg_1,Lg_2} - T_{Lg_1,Lg_2}T_{Lg_1,Lg_2}^T||_2$\n",
    "                * **Note** The Normality Regularizer is used to ensure that the resulting matrix is diagonalizable.\n",
    "\n",
    "\n",
    "#### Full cost function:\n",
    "$$ J(T_{Lg_1,Lg_2})= Loss(T_{Lg_1,Lg_2}) + \\lambda_{1}Reg_{Frobenius}(T_{Lg_1,Lg_2}) + \\lambda_{2}Reg_{Normality}(T_{Lg_1,Lg_2}) $$  \n",
    "\n",
    "### **(Stage 3)** Translation Spectral Analysis:\n",
    "* Factor the matrix $T_{Lg_1,Lg_2} = U\\Sigma V^T$ where $U$ and $V$ are orthonormal (rotation) matrices and $\\Sigma$ gives the eigenvalues of $T_{Lg_1,Lg_2}$  or the \"*Translation spectrum*\"\n",
    "* Run a statistical analysis of the spectral values associated with each pair of languages.\n",
    "    1. mean\n",
    "    2. median\n",
    "    3. max value\n",
    "    4. min value\n",
    "    5. standard deviation\n",
    "\n",
    "* Compare the statistical spectral analysis across different language pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Stage 0) Prepare vocabulary:\n",
    "\n",
    "# TODO \n",
    " * add details on where the vocab was downloaded from\n",
    " * point to where the data is in the repo\n",
    " * add instructions on where to call it in the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Stages 1 and 2) \n",
    "1. Word embeddings. Using fasttext or word2vec we embed the vocab for each of the languages\n",
    "2. Train translation matrices:\n",
    "\n",
    "# TODO \n",
    " * add details on where the vocab is located\n",
    " * point to where the embeddings are located\n",
    " * Test following code on embedding process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import tools for running Spectral decomposition\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import ismtools\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO script this section and call the script from bash in this cell\n",
    "    # add the -a for setting calculate_KNN = True\n",
    "# TODO?? put manual list of translations into ismtools or another imported .py\n",
    "\n",
    "#Set parameter on calculate KNN (need to change for -a sysarg)\n",
    "calculate_KNN = False\n",
    "\n",
    "languages = ['en','ru','de','es','fr','it', 'zh-CN']\n",
    "for lang1 in languages:\n",
    "    for lang2 in languages:\n",
    "        translations.append(('fasttext_top',lang1, lang2))\n",
    "\n",
    "for translation in translations:\n",
    "    embedding, lg1, lg2 = translation\n",
    "    # Vocab/Vectors/Dicts\n",
    "    lg1_vocab, lg1_vectors, lg2_vocab, lg2_vectors = \\\n",
    "        pickle_rw((lg1 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                  (lg1 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                  (lg2 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                  (lg2 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                  write=False)\n",
    "    lg1_dict = make_dict(lg1_vocab, lg1_vectors)\n",
    "    lg2_dict = make_dict(lg2_vocab, lg2_vectors)\n",
    "\n",
    "    print('Translation: '+lg1+'->'+lg2+'\\n')\n",
    "\n",
    "    # Train/Test Vocab/Vectors\n",
    "    vocab_train, vocab_test = vocab_train_test(embedding, lg1, lg2, lg1_vocab)\n",
    "    X_train, X_test, y_train, y_test = vectors_train_test(vocab_train,\n",
    "                                                          vocab_test,lg1_dict,lg2_dict)\n",
    "    \n",
    "    # Fit tranlation matrix to training data\n",
    "    model, history, T, tf, I, M, fro = translation_matrix(X_train, y_train)\n",
    "    \n",
    "    if calculate_KNN:\n",
    "        results_df = translation_results(X_test, y_test, vocab_test, T,\n",
    "                                     lg2_vectors, lg2_vocab)\n",
    "        acc = T_norm_EDA(results_df)\n",
    "\n",
    "\"\"\"\n",
    "TODO create standardized dumping location for the translation matrix and its meta data.\n",
    "- pickel dump based on the dir structure in the folders\n",
    "\"\"\"        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Stage 3) Translation Spectral Analysis\n",
    "\n",
    "# TODO \n",
    " * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./T_Matrices_examples/T_matrix_de_es.pkl\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-85616075e742>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"./T_Matrices_examples/T_matrix_de_es.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mobject_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \"\"\"\n",
      "\u001b[0;32m/Users/mtamir/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mtamir/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mtamir/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload_eof\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_eof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_eof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set list of T_matrix full paths\n",
    "T_matrix_dir = \"./T_Matrices_examples/\"\n",
    "T_matrix_names = !ls $T_matrix_dir\n",
    "\n",
    "T_matrix_full_paths = []\n",
    "for T_matrix_name in T_matrix_names:\n",
    "    T_matrix_loc = T_matrix_dir+T_matrix_name\n",
    "    T_matrix_full_paths += [T_matrix_loc]\n",
    "\n",
    "# Load T_matrix into memory\n",
    "load_path = T_matrix_full_paths[2]\n",
    "print(load_path)\n",
    "file = open (\"./T_Matrices_examples/T_matrix_de_es.pkl\",'rb')\n",
    "file.seek(0)\n",
    "object_file = pickle.load(file)\n",
    "\n",
    "\"\"\"\n",
    "with open(r\"./T_Matrices_examples/T_matrix_de_es.pkl\", 'rb') as f:\n",
    "    T_matrix = pickle.load(f)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unsupported pickle protocol: 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-9d6150b365db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./T_Matrices_examples/T_matrix_de_fr.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/mtamir/anaconda/lib/python2.7/site-packages/pandas/io/pickle.pyc\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(path, compression)\u001b[0m\n\u001b[1;32m     92\u001b[0m                     lambda f: pc.load(f, encoding=encoding, compat=True))\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mtamir/anaconda/lib/python2.7/site-packages/pandas/io/pickle.pyc\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 return read_wrapper(\n\u001b[0;32m---> 92\u001b[0;31m                     lambda f: pc.load(f, encoding=encoding, compat=True))\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mtamir/anaconda/lib/python2.7/site-packages/pandas/io/pickle.pyc\u001b[0m in \u001b[0;36mread_wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m     66\u001b[0m                             is_text=False)\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_f\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mtamir/anaconda/lib/python2.7/site-packages/pandas/io/pickle.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 return read_wrapper(\n\u001b[0;32m---> 92\u001b[0;31m                     lambda f: pc.load(f, encoding=encoding, compat=True))\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mtamir/anaconda/lib/python2.7/site-packages/pandas/compat/pickle_compat.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(fh, encoding, compat, is_verbose)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_verbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mtamir/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mtamir/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload_proto\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0mproto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mproto\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unsupported pickle protocol: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPROTO\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unsupported pickle protocol: 3"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_pickle(\"./T_Matrices_examples/T_matrix_de_fr.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U,s,Vh = SVD(T)\n",
    "\n",
    "s1 = log(s)\n",
    "\n",
    "\n",
    "for stat in stats:\n",
    "    svd[stat,translation[1],translation[2]] = stat_calc(stat, s, fro, acc)\n",
    "    svd1[stat,translation[1],translation[2]] = stat_calc(stat, s1, fro, acc)\n",
    "\n",
    "\n",
    "#Exporting DataFrames for SVD Heatmaps\n",
    "\n",
    "s_df = make_df(languages,languages)\n",
    "s1_df = make_df(languages,languages)\n",
    "\n",
    "\n",
    "for stat in stats:\n",
    "    for lang1 in languages:\n",
    "        for lang2 in languages:\n",
    "            s_df.set_value(lang1,lang2,svd[stat,lang1,lang2])\n",
    "            s1_df.set_value(lang1,lang2,svd1[stat,lang1,lang2])\n",
    "\n",
    "    s_df.to_csv('../HeatmapData/T/s_{}.csv'.format(stat),columns = languages)\n",
    "    s1_df.to_csv('../HeatmapData/T/s1_{}.csv'.format(stat),columns = languages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Manually set list of translations (embedding, lg1, lg2)\n",
    "    \n",
    "    svds = ['s','s1']\n",
    "    languages = ['en','ru','de','es','fr','it', 'zh-CN']\n",
    "    stats = ['min','max','mean','median','std','fro','acc']\n",
    "    \n",
    "    translations=[]\n",
    "    \n",
    "    for lang1 in languages:\n",
    "        for lang2 in languages:\n",
    "            translations.append(('fasttext_top',lang1, lang2))\n",
    "    \n",
    "    svd = {}\n",
    "    svd1 = {}\n",
    "    \n",
    "    for translation in translations:\n",
    "        embedding, lg1, lg2 = translation\n",
    "        # Vocab/Vectors/Dicts\n",
    "        lg1_vocab, lg1_vectors, lg2_vocab, lg2_vectors = \\\n",
    "            pickle_rw((lg1 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                      (lg1 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                      (lg2 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                      (lg2 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                      write=False)\n",
    "        lg1_dict = make_dict(lg1_vocab, lg1_vectors)\n",
    "        lg2_dict = make_dict(lg2_vocab, lg2_vectors)\n",
    "\n",
    "        print('Translation: '+lg1+'->'+lg2+'\\n')\n",
    "\n",
    "        # Train/Test Vocab/Vectors\n",
    "        vocab_train, vocab_test = vocab_train_test(embedding, lg1, lg2, lg1_vocab)\n",
    "        X_train, X_test, y_train, y_test = vectors_train_test(vocab_train,\n",
    "                                                              vocab_test,lg1_dict,lg2_dict)\n",
    " \n",
    "        \n",
    "        # Fit tranlation matrix to training data\n",
    "        model, history, T, tf,I, M, fro = translation_matrix(X_train, y_train)\n",
    "        \n",
    "        results_df = translation_results(X_test, y_test, vocab_test, T,\n",
    "                                         lg2_vectors, lg2_vocab)\n",
    "        acc = T_norm_EDA(results_df)\n",
    "        \n",
    "        U,s,Vh = SVD(T)\n",
    "        \n",
    "        s1 = log(s)\n",
    "    \n",
    "        \n",
    "        for stat in stats:\n",
    "            svd[stat,translation[1],translation[2]] = stat_calc(stat, s, fro, acc)\n",
    "            svd1[stat,translation[1],translation[2]] = stat_calc(stat, s1, fro, acc)\n",
    "            \n",
    "        \n",
    "    #Exporting DataFrames for SVD Heatmaps\n",
    "    \n",
    "    s_df = make_df(languages,languages)\n",
    "    s1_df = make_df(languages,languages)\n",
    "    \n",
    "\n",
    "    for stat in stats:\n",
    "        for lang1 in languages:\n",
    "            for lang2 in languages:\n",
    "                s_df.set_value(lang1,lang2,svd[stat,lang1,lang2])\n",
    "                s1_df.set_value(lang1,lang2,svd1[stat,lang1,lang2])\n",
    "\n",
    "        s_df.to_csv('../HeatmapData/T/s_{}.csv'.format(stat),columns = languages)\n",
    "        s1_df.to_csv('../HeatmapData/T/s1_{}.csv'.format(stat),columns = languages)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Translation Matrix Results  \n",
    "## En to Ru Fasttext_Random  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- Ru Vocabulary Size = 944,211  \n",
    "- Ru Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 3.9%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for Ru test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_ru_fasttext_random_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 32.3%  \n",
    "![](../images/en_ru_fasttext_random_T_isotropy.png)  \n",
    "\n",
    "## En to Ru Fasttext_Top  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- Ru Vocabulary Size = 944,211  \n",
    "- Ru Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 46.3%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for Ru test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_ru_fasttext_top_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 38.2%  \n",
    "![](../images/en_ru_fasttext_top_T_isotropy.png)  \n",
    "\n",
    "## En to De Fasttext_Random  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- De Vocabulary Size = 1,137,616  \n",
    "- De Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 21.9%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for De test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_de_fasttext_random_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 35.6%  \n",
    "![](../images/en_de_fasttext_random_T_isotropy.png)  \n",
    "\n",
    "## En to De Fasttext_Top  \n",
    "- En Vocabulary Size = 1,259,685  \n",
    "- En Embedding Length = 300  \n",
    "- De Vocabulary Size = 1,137,616  \n",
    "- De Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,500  \n",
    "- <b>Test Accuracy = 63.6%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for De test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_de_fasttext_top_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 43.4%  \n",
    "![](../images/en_de_fasttext_top_T_isotropy.png)  \n",
    "\n",
    "## En to It Zeroshot  \n",
    "- En Vocabulary Size = 200,000  \n",
    "- En Embedding Length = 300  \n",
    "- It Vocabulary Size = 200,000  \n",
    "- It Embedding Length = 300  \n",
    "- Train Size = 5,000  \n",
    "- Test Size = 1,869  \n",
    "- <b>Test Accuracy = 27.9%</b>  \n",
    "\n",
    "#### Test L2 Norms  \n",
    "- X_norm: L2 norms for En test vectors  \n",
    "- y_norm: L2 norms for It test vectors  \n",
    "- yhat_norm: L2 norms for X.dot(T) test vectors (T = translation matrix)  \n",
    "- yhat_neighbor norm: L2 norms for nearest neighborto X.dot(T) in y test vectors  \n",
    "![](../images/en_it_zeroshot_T_norm.png)  \n",
    "\n",
    "#### Translation Matrix Isotropy  \n",
    "- Isotropy = 46.6%  \n",
    "![](../images/en_it_zeroshot_T_isotropy.png)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
